"""Dispatcher ‚Äî ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞ route message ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô
   - /command ‚Üí tool ‡∏ï‡∏£‡∏á (‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏µ‡∏¢ token)
   - ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¥‡∏™‡∏£‡∏∞ ‚Üí LLM Router ‚Üí function calling ‚Üí tool
   - general chat ‚Üí LLM ‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á
"""

import asyncio
from core.llm import llm_router
from core.memory import save_user_message, save_assistant_message, get_context
from core.user_manager import get_preference, set_preference
from core import db
from tools.registry import registry
from interfaces.telegram_common import parse_command
from core.logger import get_logger

log = get_logger(__name__)

SYSTEM_PROMPT = (
    "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß ‡∏ä‡∏∑‡πà‡∏≠ OpenMiniCrew ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏° "
    "‡∏ñ‡πâ‡∏≤ user ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö tool ‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ tool ‡∏ô‡∏±‡πâ‡∏ô "
    "‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö tool ‡πÑ‡∏´‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"
)


async def dispatch(user_id: str, user: dict, text: str) -> str:
    """
    ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à route message ‚Üí return ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö user

    Returns:
        (response_text, tool_used, llm_model, token_used)
    """
    command, args = parse_command(text)

    # ---- /help ----
    if command == "/help":
        return registry.get_help_text(), None, None, 0

    # ---- /model ----
    if command == "/model":
        return _handle_model_command(user_id, args), None, None, 0

    # ---- Direct command ‚Üí tool (‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏µ‡∏¢ LLM token) ----
    tool = registry.get_by_command(command) if command else None
    if tool:
        log.info(f"Direct command: {command} ‚Üí {tool.name}")
        try:
            result = await tool.execute(user_id, args)
            return result, tool.name, None, 0
        except Exception as e:
            log.error(f"Tool {tool.name} failed: {e}")
            db.log_tool_usage(user_id, tool.name, status="failed", error_message=str(e))
            return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà", tool.name, None, 0

    # ---- ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¥‡∏™‡∏£‡∏∞ ‚Üí LLM Router ----
    provider = get_preference(user, "default_llm")
    context = get_context(user_id)
    context.append({"role": "user", "content": text})

    tool_specs = registry.get_all_specs()

    try:
        # Step 1: ‡∏ñ‡∏≤‡∏° LLM ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å tool ‡πÑ‡∏´‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏≠‡∏ö‡πÄ‡∏≠‡∏á
        resp = await llm_router.chat(
            messages=context,
            provider=provider,
            tier="cheap",
            system=SYSTEM_PROMPT,
            tools=tool_specs if tool_specs else None,
        )

        # Step 2: ‡∏ñ‡πâ‡∏≤ LLM ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å tool ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å tool ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡∏∏‡∏õ
        if resp["tool_call"]:
            tool_name = resp["tool_call"]["name"]
            tool_args = resp["tool_call"].get("args", {})
            selected_tool = registry.get_tool(tool_name)

            if selected_tool:
                log.info(f"LLM selected tool: {tool_name}")
                try:
                    raw_args = tool_args.get("args", "")
                    if not isinstance(raw_args, str):
                        raw_args = str(raw_args)
                    tool_result = await selected_tool.execute(user_id, raw_args)

                    # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå tool ‡∏Å‡∏•‡∏±‡∏ö‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
                    summary_messages = context + [
                        {"role": "assistant", "content": f"[‡πÄ‡∏£‡∏µ‡∏¢‡∏Å {tool_name}]"},
                        {"role": "user", "content": f"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å {tool_name}:\n{tool_result}\n\n‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ user ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢"},
                    ]

                    summary = await llm_router.chat(
                        messages=summary_messages,
                        provider=provider,
                        tier="cheap",
                        system=SYSTEM_PROMPT,
                    )

                    total_tokens = resp["token_used"] + summary["token_used"]
                    return (
                        summary["content"],
                        tool_name,
                        summary["model"],
                        total_tokens,
                    )

                except Exception as e:
                    log.error(f"Tool {tool_name} execution failed: {e}")
                    db.log_tool_usage(user_id, tool_name, status="failed",
                                      error_message=str(e))
                    return f"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å {tool_name} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}", tool_name, resp["model"], resp["token_used"]
            else:
                log.warning(f"LLM selected unknown tool: {tool_name}")

        # Step 3: ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å tool ‚Üí ‡πÉ‡∏ä‡πâ text response ‡∏ï‡∏£‡∏á
        return resp["content"], None, resp["model"], resp["token_used"]

    except Exception as e:
        log.error(f"LLM dispatch failed: {e}")
        return f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {e}\n‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà", None, None, 0


def _handle_model_command(user_id: str, args: str) -> str:
    from core.llm import llm_router

    available = llm_router.get_available_providers()
    args = args.strip().lower()

    if not args:
        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ providers ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
        lines = ["üß† LLM ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ:\n"]
        for name in available:
            lines.append(f"  ‚úÖ {name}")
        lines.append(f"\n‡πÉ‡∏ä‡πâ: /model [‡∏ä‡∏∑‡πà‡∏≠] ‡πÄ‡∏ä‡πà‡∏ô /model {available[0]}" if available else "\n‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ LLM ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ")
        return "\n".join(lines)

    if args not in available:
        return f"‚ùå ‡πÉ‡∏ä‡πâ {args} ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚Äî ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API key\n‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ: {', '.join(available)}"

    set_preference(user_id, "default_llm", args)
    return f"‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô LLM ‡πÄ‡∏õ‡πá‡∏ô {args} ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢"


async def process_message(user_id: str, user: dict, chat_id: str | int, text: str):
    """
    Full pipeline: dispatch + save memory + send Telegram
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á polling ‡πÅ‡∏•‡∏∞ webhook
    """
    from interfaces.telegram_common import send_message, TypingIndicator

    with TypingIndicator(chat_id):
        save_user_message(user_id, text)

        result = await dispatch(user_id, user, text)

        if isinstance(result, tuple):
            response_text, tool_used, llm_model, token_used = result
        else:
            response_text, tool_used, llm_model, token_used = result, None, None, 0

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å memory
        save_assistant_message(
            user_id, response_text,
            tool_used=tool_used, llm_model=llm_model,
            token_used=token_used,
        )

    # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö Telegram (‡∏´‡∏•‡∏±‡∏á typing ‡∏´‡∏¢‡∏∏‡∏î)
    send_message(chat_id, response_text)
